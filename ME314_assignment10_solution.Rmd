---
title: "Assignment 10 - Topic Models (Solutions)"
author: "Jack Blumenau"
output: html_document
---

You will need to load the following libraries (you may also want to set the random number seed to make everything replicable):
```{r, eval=T, message = F}
library(quanteda)
library(topicmodels)
library(LDAvis)
library(stm)
library(knitr)
library(lda)
library(servr)
set.seed(221186)
```

## Topic modelling of parliamentary speeches

In this question we are going to use topic modelling to understand how parliamentary speech varies by the gender of the MP. We will be working with a corpus of speeches made by legislators in the UK House of Commons in the 2014 calandar year. 

You will need to make sure that the file `hoc_speeches.Rdata` is in your current working directory, and then use the following command to read this data into `R`.


```{r, message = FALSE}

load("hoc_speeches.Rdata")

```
 
 (a) Inspect the `data.frame` object `speeches` and produce some summary statistics.

```{r}

prop.table(table(speeches$party, speeches$gender),1)

speeches$ntoken <- ntoken(speeches$speech)
hist(speeches$ntoken, main = "Distribution of speech length", breaks = 100)

```

 (a) Use the functions in the `quanteda` package to turn this data into a `corpus` object. Attach the relevant metadata as `docvars`.

```{r}

speechCorpus <- corpus(speeches$speech, docvars = speeches)

```

 (b) Turn this corpus into a document-feature matrix. At a minimum, you should remove punctuation and numbers from the texts when constructing the `dfm` (`remove_punct =T` & `remove_numbers = T`) but you may also want to do some additional pre-processing if you don't want to wait days for your topic model to coverge. Think about some of the following:
 
    (i) Unigrams? 
    (ii) Stopwords?
    (iii) Stemming?
    (iv) Very infrequent words?

```{r, message = FALSE}

speechDFM <- dfm(speechCorpus, remove = stopwords("en"), remove_punct =T, remove_numbers = T, stem = T)

speechDFM <- dfm_trim(speechDFM, min_termfreq = 5, min_docfreq = 0.0025, docfreq_type = "prop")

```

 (c) Run a structural topic model for this corpus, using the `gender` variable in the topic prevalence argument. Use the `stm` function to do this. Set the `seed` argument to `stm` to be equal to `123`. Be aware, this takes about 15 minutes to run on Jack's laptop -- for testing purposes you might want to set the maximum iterations for the stm to be some low number (`max.em.its = 10` for instance).

Specify and estimate the `stm` model:

```{r, cache = T, message=FALSE}

K <- 20
stmOut <- stm(documents = speechDFM, 
              data = docvars(speechDFM),
              prevalence = ~gender,
              K = K, seed = 123, verbose = FALSE, max.em.its = 500)

```

Plot the estimated topic model:

```{r}
plot(stmOut)
```

  (d) Examine the top words from each topic

```{r}

topic_labels <- labelTopics(stmOut)
topic_labels <- apply(topic_labels$prob,1, function(x) paste(x, collapse=";"))
print(topic_labels)
```


  (e) Find the top three documents associated with each topic. Do these make sense given the words you have used to describe that topic? (Hint: in the estimated `stm` object, the document-topic probabilities are stored in `theta`) Report the top speeches for one selected topic.
  
```{r}

top_docs <- apply(stmOut$theta, 2, function(x) order(x, decreasing = T)[1:3])

top_school_docs <- top_docs[,grep("school",topic_labels)]

docvars(speechDFM)[top_school_docs,"speech"]

```

  (f) Use the `estimateEffect` and `plot.estimateEffect` functions in the `stm` package to estimate the effect of MP gender on topic usage. On which topics are women, on average, more active? 

```{r}

est_gender_effect <- estimateEffect(~gender, stmOut, metadata = docvars(speechDFM))

plot.estimateEffect(est_gender_effect, "gender", method = "difference", 
                    cov.value1 = "female", cov.value2 = "male", 
                    labeltype = "frex", n = 3, verbose.labels = F,
                    model = stmOut)

```

**Women appear to speak more about the `job,wage,economi`, and `nhs,patient,cancer` topics, though the significance of these is effects in this data is questionable.**

## Topic modelling of movie reviews

2.  **movies corpus**.  Here we will use the very impressive `LDAvis` library in conjunction with the `lda::lda.collapsed.gibbs.sampler()` function from the `lda` package. The following code is used to demonstate how the parliamentary speeches interactive visualisation example was created for in the lecture. Your task is to implement this for the `movies` corpus.

First we construct the relevant `dfm` and estimate the `lda` model.
```{r, eval=FALSE}

## Create a corpus of speeches
speechCorpus <- corpus(speeches$speech)

## Convert to dfm, removing some words that appear very regularly
speechDfm <- dfm(speechCorpus, remove = c(stopwords("en"), "will", "hon", "right","people","government","can","friend","house","gentleman","said", "interruption", "prime", "minister", "secretary", "state"), stem = F, remove_punct = T, remove_numbers = T)

## Trim some rarely occuring words
speechDfm <- dfm_trim(speechDfm, min_termfreq = 15, min_docfreq = 0.0015, docfreq_type = "prop")

# Convert to lda format
speechDfmlda <- convert(speechDfm, to = "lda")

# MCMC and model tuning parameters:
K <- 30 # Number of topics
G <- 2000 # Number of iterations
alpha <- 0.02 # Prior for topic proportions
eta <- 0.02 # Prior for topic distributions

# Fit the model
t1 <- Sys.time() # Start timer

fit <- lda.collapsed.gibbs.sampler(documents = speechDfmlda$documents, K = K, 
                                   vocab = speechDfmlda$vocab, 
                                   num.iterations = G, alpha = alpha, 
                                   eta = eta, initial = NULL, burnin = 0,
                                   compute.log.likelihood = TRUE)
t2 <- Sys.time() # End timer
t2 - t1  # about 15 minutes on Jack's MacBook Pro


```

Now we plot the model using `LDAvis`.

```{r, eval=FALSE}
library(LDAvis)
# create the JSON object to feed the visualization:
json <- createJSON(phi = t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x))), 
                   theta = t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x))), 
                   doc.length = ntoken(speechDfm), 
                   vocab = featnames(speechDfm), 
                   term.frequency = colSums(speechDfm))
        
serVis(json, out.dir = "exampleVis", open.browser = TRUE)
```

  a.  You will need to load the data from the `quanteda.corpora` package which is hosted on GitHub: 
    
```{r}
  library(devtools)
  #install_github("quanteda/quanteda.corpora")
  data(data_corpus_movies, package = "quanteda.corpora")
  
```
    
  b.  Adapt the code above to produce an interactive visualisation of the `movies` corpus. 
    
```{r, cache = TRUE, eval = FALSE}

      
# prepare the texts
moviesDfm <- dfm(data_corpus_movies, remove = stopwords("en"), stem = FALSE, remove_punct = T, remove_numbers = T)
moviesDfm <- dfm_trim(moviesDfm, min_termfreq = 5)

# MCMC and model tuning parameters:
K <- 20
G <- 2000
alpha <- 0.02
eta <- 0.02

# convert to lda format
moviesDfmlda <- convert(moviesDfm, to = "lda")
# fit the model
library(lda)
set.seed(357)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = moviesDfmlda$documents, K = K, 
                                   vocab = moviesDfmlda$vocab, 
                                   num.iterations = G, alpha = alpha, 
                                   eta = eta, initial = NULL, burnin = 0,
                                   compute.log.likelihood = TRUE)
t2 <- Sys.time()
t2 - t1  # about 10 minutes on Jack's iMac

library(LDAvis)
# create the JSON object to feed the visualization:
json <- createJSON(phi = t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x))), 
                   theta = t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x))), 
                   doc.length = ntoken(moviesDfm), 
                   vocab = featnames(moviesDfm), 
                   term.frequency = colSums(moviesDfm))

serVis(json, out.dir = "visColl", open.browser = TRUE)

```
    
  d.  Describe a few topics as you see them.  Is there a "scary movie" topic?  Is there a "science fiction" topic?  
    
    